## 牛顿法与拟牛顿法

### 1. 牛顿法

**无约束最优化问题**
![\begin{align*} \\& \min_{x \in R^{n}} f\left(x\right)\end{align*} \\](http://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5C%5C%26+%5Cmin_%7Bx+%5Cin+R%5E%7Bn%7D%7D+f%5Cleft%28x%5Cright%29%5Cend%7Balign%2A%7D+%5C%5C)

其中 ![x^{*}](http://www.zhihu.com/equation?tex=x%5E%7B%2A%7D) 为目标函数的极小点。

​	设 ![f\left(x\right)](http://www.zhihu.com/equation?tex=f%5Cleft%28x%5Cright%29) 具有二阶连续偏导数，若第 ![k](http://www.zhihu.com/equation?tex=k) 次迭代值为 ![x^{\left(k\right)}](http://www.zhihu.com/equation?tex=x%5E%7B%5Cleft%28k%5Cright%29%7D) ，则可将 ![f\left(x\right)](http://www.zhihu.com/equation?tex=f%5Cleft%28x%5Cright%29) 在 ![x^{\left(k\right)}](http://www.zhihu.com/equation?tex=x%5E%7B%5Cleft%28k%5Cright%29%7D) 附近进行二阶泰勒展开

$$f(x)=f(x^{(k)})+g^T_k(x-x^{(k)})+\frac{1}{2!}(x-x^{(k)})^T H(x^{(k)}) (x-x^{(k)})\ \ \ \ \ \ \ \ \ \ ...2.1$$ 

其中， ![g_{k}=g\left(x^{\left(k\right)}\right)=\nabla f\left(x^{\left(k\right)}\right)](http://www.zhihu.com/equation?tex=g_%7Bk%7D%3Dg%5Cleft%28x%5E%7B%5Cleft%28k%5Cright%29%7D%5Cright%29%3D%5Cnabla+f%5Cleft%28x%5E%7B%5Cleft%28k%5Cright%29%7D%5Cright%29) 是 ![f\left(x\right)](http://www.zhihu.com/equation?tex=f%5Cleft%28x%5Cright%29) 的梯度向量在点 ![x^{\left(k\right)}](http://www.zhihu.com/equation?tex=x%5E%7B%5Cleft%28k%5Cright%29%7D) 的值， ![H\left(x^{\left(k\right)}\right)](http://www.zhihu.com/equation?tex=H%5Cleft%28x%5E%7B%5Cleft%28k%5Cright%29%7D%5Cright%29) 是 ![f\left(x\right)](http://www.zhihu.com/equation?tex=f%5Cleft%28x%5Cright%29) 的海赛矩阵
![\begin{align*} \\& H\left(x\right)=\left[\dfrac{\partial^{2}f}{\partial x_{i} \partial x_{j}}\right]_{n \times n}\end{align*} \\](http://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5C%5C%26+H%5Cleft%28x%5Cright%29%3D%5Cleft%5B%5Cdfrac%7B%5Cpartial%5E%7B2%7Df%7D%7B%5Cpartial+x_%7Bi%7D+%5Cpartial+x_%7Bj%7D%7D%5Cright%5D_%7Bn+%5Ctimes+n%7D%5Cend%7Balign%2A%7D+%5C%5C)

在点 ![x^{\left(k\right)}](http://www.zhihu.com/equation?tex=x%5E%7B%5Cleft%28k%5Cright%29%7D) 的值。

​	**函数 ![f\left(x\right)](http://www.zhihu.com/equation?tex=f%5Cleft%28x%5Cright%29) 有极值的必要条件是在极值点处一阶导数为0，即梯度向量为0。特别的当 ![H\left(x^{\left(k\right)}\right)](http://www.zhihu.com/equation?tex=H%5Cleft%28x%5E%7B%5Cleft%28k%5Cright%29%7D%5Cright%29)是正定矩阵时，函数 ![f\left(x\right)](http://www.zhihu.com/equation?tex=f%5Cleft%28x%5Cright%29) 的极值为极小值。**

​	根据二阶泰勒展开，对 ![\nabla f(x)](http://www.zhihu.com/equation?tex=%5Cnabla+f%28x%29)在 ![x^{\left(k\right)}](http://www.zhihu.com/equation?tex=x%5E%7B%5Cleft%28k%5Cright%29%7D) 进行展开得（也可以对(1.1)再进行求导)：

$$\nabla f(x)=g_k+H_k(x-x^{(k)})\ \ \ \ \ \ \ \ \ \ ...1.2$$

​	则令其为0可得：$x=x^{(k)}-H^{-1}_kg_k$

​	**即得到迭代公式：**$x^{(k+1)}=x^{(k)}-H^{-1}_kg_k\ \ \ \ \ \ \ \ \ \ ...1.3$

​	令 $H_kp_k=-g_k$,

​	**则得到迭代公式：**

​								$$x^{(k+1)}=x^{(k)}+p_k$$

​	最终可在 ![\nabla f(x^*)=0](http://www.zhihu.com/equation?tex=%5Cnabla+f%28x%5E%2A%29%3D0) 收敛。



####  算法

> **牛顿法**：
> 输入：目标函数$f(x)$，梯度$g(x)=\nabla f(x)$，海赛矩阵$H(x)$，精度要求$\epsilon$
> 输出：$f(x)$的极小点$x^{*}$
>
> 1. 取初始点$x^{\left(0\right)}$，置$k=0$
> 2. 计算$g_{k}=g\left(x^{\left(k\right)}\right)$ 
> 3. 若$\|g_{k}\| < \varepsilon$ 则停止计算，得近似解 $x^{*}=x^{\left(k\right)}$
> 4. 计算 $H_{k}=H\left(x^{\left(k\right)}\right)$，并求 $p_{k}$
>    $\begin{align*} \\& H_{k}p_{k}=-g_{k}\end{align*}\\$
> 5. 置 $x^{\left(k+1\right)}=x^{\left(k\right)}+p_{k}$
> 6. 置 $k=k+1$，转2。

#### 帮助理解

**$f(x)$即为损失函数或代价函数。**

**基本思想是：**根据泰勒公式得到$x$附近某个点$x_k$展开的多项式可用来近似函数$f(x)$的值，该多项式对应的函数为$g(x)$，求得$g(x)$的极小值(这里需要令一阶导数为$0$)作为新的迭代点，然后继续在新的迭代点泰勒公式展开，直到求得的极小值满足一定的精度，则可认为此时的$x^*$为能使$f(x)$等于极小值的点。

**原理:**
假设函数$f(x)$二次可微，则二次泰勒展开， 
$$f(x)≈g(x)=f(x_k)+f′(x_k)(x−x_k)+\frac{1}{2!}f″(x_k)(x−x_k)^2$$

$g(x)$多项式则为$f(x)$的泰勒展开近似，求函数$f(x)$极值则可以转化为求导函数为0，对$g(x)$求导并令其为0， 即得：
$$f′(x_k)+f″(x_k)(x−x_k)=0$$

得到， 
$$x=x_k−f′(x_k)f″(x_k)$$

即得到迭代公式， 
$$x_{k+1}=x_k−f′(x_k)f″(x_k)$$

新的点$x_{k+1}$不断逼近极值，直到一次导数小于某误差。

#### 优缺点

优点：收敛快，迭代次数少；

缺点：海塞矩阵和其逆矩阵，存储空间大，计算资源大；

### 2. 拟牛顿法

**基本想法：**用一个n阶矩阵$G_k＝G(x^{(k)})$来近似代替$H_k=H(x^{(k)})$。

#### 海塞矩阵满足的条件

**1. 拟牛顿条件**：

$$\nabla f(x)=g_k+H_k(x-x^{(k)})\ \ \ \ \ \ \ \ \ \ ...1.2$$

令$x=x^{k+1}$,可得：

$$\nabla f(x+1)=g_k+H_k(x^{(k+1)}-x^{(k)})$$

$$=> g_{k+1}=g_k+H_k(x^{(k+1)}-x^{(k)})$$

$$=>  g_{k+1}-g_k=H_k(x^{(k+1)}-x^{(k)})\ \ \ \ \ \ \ \ \ \ \ \ ...2.1$$

令$y_k=g_{k+1}-g_k, \delta_k=x^{(k+1)}-x^{(k)}$,则得到拟牛顿条件：

$y_k=H_k \delta_k\ \ \ \ ...2.2$      或         $H_k^{-1} y_k=\delta_k\ \ \ \ ...2.3$ 

**2. 正定矩阵：**

如果$H_k$是正定的（$H_k^{-1}$也是正定的），那么可以保证牛顿法搜索方向$p_k$是下降方向。这是因为搜索方向是$p_k=-\lambda g_k$，由式$x^{(k+1)}=x^{(k)}-H^{-1}_kg_k$有：

$$x=x^{(k)}+\lambda p_k=x^{(k)}-\lambda H_k^{-1}g_k$$

所以$f(x)$在$x^{(k)}$的泰勒展开式可以近似写成：

$$f(x)=f(x^{(k)})--\lambda g_k^{-1}H_k^{-1}g_k$$

因为$H_k^{-1}$正定，故有$g_k^{-1}H_k^{-1}g_k$。

当$\lambda$为一个充分小的正数时，总有$f(x)<f(x^{(k)})$，也就是说$p_k$是下降方向。

> $G_k$ 近似$H_k^{-1}$ 需满足条件： 
>
> 1. 正定矩阵；
> 2. $G_{k+1}y_k=\delta_k$

**按照拟牛顿条件选择$G_k$作为$H_k^{-1}$的近似或选择$B_k$作为$H_k$的近似的算法称为拟牛顿法。**
**按照拟牛顿条件，在每次迭代中可以选择更新矩阵$G_{k+1}$：**

$$G_{k+1}=G_k+\Delta G$$



#### $DFP$算法

$DFP（Davidon-Fletcher-Powell）$算法，**选择近似矩阵的方法是，用$G_k$近似海塞矩阵的逆矩阵$H_k^{-1}$。**

假设每一步迭代中矩阵$G_{k+1}$是由$G_k$加上两个附加项构成的，即

$$G_{k+1}=G_k+P_k+Q_k$$

其中$P_k,Q_k$是待定矩阵。这时，

$$G_{k+1}y_k=G_ky_k+P_ky_k+Q_ky_k$$

为使$G_{k+1}$满足拟牛顿条件，可使$P_k$和$Q_k$满足：

$$P_ky_k=\delta_k$$

$$Q_ky_k=-G_ky_k$$

事实上，不难找出这样的$P_k$和$Q_k$，例如取

$$P_k=\frac{\delta_k \delta_k^T}{\delta_k^T y_k}$$

$$Q_k=-\frac{G_k y_k y_k^T G_k}{y_k^T G_k y_k}$$

这样就可得到矩阵$G_{k+1}$的迭代公式：

$$G_{k+1}=G_k+\frac{\delta_k \delta_k^T}{\delta_k^T y_k}-\frac{G_k y_k y_k^T G_k}{y_k^T G_k y_k}$$

称为DFP算法。

可以证明，如果初始矩阵$G_0$是正定的，则迭代过程中的每个矩阵$G_k$都是正定的。

> **$DFP$算法：**
> 输入：目标函数$f(x)$，梯度$g(x)=\nabla f(x)$，精度要求$\epsilon$
> 输出：$f(x)$的极小点
>
> 1. 取初始点$x^{(0)}$，取$G_0$为正定矩阵，置$k=0$  
>
> 2. 计算$g_k$， 若$||g_k||<\epsilon$则停止计算，得近似解$x^{*}=x^{(k)}$；否则，转3 
>
> 3. 置$p_k=-G_k g_k$  ，求出$p_k $
>
> 4. 一维搜索，求$\lambda_k$使
>
>    $$f(x^{(k)}+\lambda_k p_k)=\min\limits_{\lambda>=0}f(x^{(k)}+\lambda p_k)$$
>
> 5. 置$$x^{(k+1)}=x^{(k)}+\lambda_k p_k$$
>
> 6. 计算$g_{k+1}$，若$||g_{k+1}||<\epsilon$则停止计算，近似解$x^{*}=x^{(k+1)}$；否则，计算
>
>    $$G_{k+1}=G_k+\frac{\delta_k \delta_k^T}{\delta_k^T y_k}-\frac{G_k y_k y_k^T G_k}{y_k^T G_k y_k}$$
>
> 7. 置$k=k+1 $，转3。



#### $BFGS$算法

$BFGS（Broyden-Fletcher-Goldfarb-Shanno）$算法，**选择近似矩阵的方法是，用近似$B_k$海塞矩阵$H_k$。**

**这时，相应的拟牛顿条件：**

$$B_{k+1} \delta_k=y_k$$

同样的方法，令：

$$B_{k+1}=B_k+P_k+Q_k$$

$$B_{k+1} \delta_k=B_k \delta_k+P_k \delta_k+Q_k \delta_k$$

取：

$$P_k \delta_k=y_k$$

$$Q_k \delta_k=-B_k \delta_k$$

$BFGS$算法的$G_{k+1}$迭代公式：

$$B_{k+1}=B_k+\frac{y_k y_k^T}{y_k^T \delta_k}-\frac{B_k \delta_k \delta_k^T B_K}{\delta_k^T B_k \delta_k}$$

可以证明，如果初始矩阵$G_0$是正定的，则迭代过程中的每个矩阵$G_k$都是正定的。

> **$BFGS$算法：**
> 输入：目标函数$f(x)$，梯度$g(x)=\nabla f(x)$，精度要求$\epsilon$
> 输出：$f(x)$的极小点
>
> 1. 取初始点$x^{(0)}$，取$B_0$为正定矩阵，置$k=0$  
>
> 2. 计算$g_k$， 若$||g_k||<\epsilon$则停止计算，得近似解$x^{*}=x^{(k)}$；否则，转3 
>
> 3. 置$B_k p_k=-g_k$ ，求出$p_k $
>
> 4. 一维搜索，求$\lambda_k$使
>
>    $$f(x^{(k)}+\lambda_k p_k)=\min\limits_{\lambda>=0}f(x^{(k)}+\lambda p_k)$$
>
> 5. 置$$x^{(k+1)}=x^{(k)}+\lambda_k p_k$$
>
> 6. 计算$g_{k+1}$，若$||g_{k+1}||<\epsilon$则停止计算，近似解$x^{*}=x^{(k+1)}$；否则，计算
>
>    $$B_{k+1}=B_k+\frac{y_k y_k^T}{y_k^T \delta_k}-\frac{B_k \delta_k \delta_k^T B_K}{\delta_k^T B_k \delta_k}$$
>
> 7. 置$k=k+1 $，转3。

