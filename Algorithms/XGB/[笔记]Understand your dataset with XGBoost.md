#### 【笔记】Understand your dataset with XGBoost

------

http://xgboost.apachecn.org/cn/latest/R-package/discoverYourData.html

* Xgboost manages only numeric vectors.

* What to do when you have categorical data? —— one-hot encoding

  ```R
  head(df[,AgeCat:= as.factor(ifelse(Age > 30, "Old", "Young"))])
  ##    ID Treatment  Sex Age Improved AgeDiscret AgeCat
  ## 1: 57   Treated Male  27     Some          3  Young
  ## 2: 46   Treated Male  29     None          3  Young
  ## 3: 77   Treated Male  30     None          3  Young
  ## 4: 17   Treated Male  32   Marked          3    Old
  ## 5: 36   Treated Male  46   Marked          5    Old
  ## 6: 23   Treated Male  58   Marked          6    Old
  ```

  * These new features are highly correlated to the Age feature
  * For many machine learning algorithms, using correlated features is not a good idea. It may sometimes make prediction less accurate, and most of the time make interpretation of the model almost impossible. 
  * Fortunately, decision tree algorithms (including boosted trees) are very robust to these features. Therefore we have nothing to do to manage this situation.

* One-hot Encoding for categorical features

  ```R
  sparse_matrix <- Matrix::sparse.model.matrix(Improved~.-1, data = df)
  head(sparse_matrix)
  ## 6 x 10 sparse Matrix of class "dgCMatrix"
  ##                       
  ## 1 . 1 1 27 1 . . . . 1
  ## 2 . 1 1 29 1 . . . . 1
  ## 3 . 1 1 30 1 . . . . 1
  ## 4 . 1 1 32 1 . . . . .
  ## 5 . 1 1 46 . . 1 . . .
  ## 6 . 1 1 58 . . . 1 . .
  ```

  * Formulae Improved~.-1 used above means transform all categorical features but column Improved to binary values. The -1 is here to remove the first column which is full of 1 (this column is generated by the conversion).

* Build the model

  ```R
  st <- xgboost(data = sparse_matrix, label = output_vector, max.depth = 4, eta = 1, nthread = 2, nround = 10,objective = "binary:logistic")
  ```

  * Here you can see the numbers decrease until line 7 and then increase.

    It probably means we are overfitting. To fix that I should reduce the number of rounds to nround = 4.

* Measure feature importance

  ```R
  importance <- xgb.importance(feature_names = sparse_matrix@Dimnames[[2]], model = bst)
  head(importance)
  
  ##             Feature        Gain      Cover  Frequency
  ## 1:              Age 0.622031651 0.67251706 0.67241379
  ## 2: TreatmentPlacebo 0.285750607 0.11916656 0.10344828
  ## 3:          SexMale 0.048744054 0.04522027 0.08620690
  ## 4:      AgeDiscret6 0.016604647 0.04784637 0.05172414
  ## 5:      AgeDiscret3 0.016373791 0.08028939 0.05172414
  ## 6:      AgeDiscret4 0.009270558 0.02858801 0.01724138
  ```

  * 用的one-hot后的特征作为模型特征，非原始特征。

  * The column Gain provide the information we are looking for.

    As you can see, features are classified by Gain.

  * Frequency is a simpler way to measure the Gain. It just counts the number of times a feature is used in all generated trees. You should not use it (unless you know why you want to use it).

  * Cover measures the relative quantity of observations concerned by a feature.

* Improvement in the interpretability of feature importance data.table

  ```R
  importanceRaw <- xgb.importance(feature_names = sparse_matrix@Dimnames[[2]], model = bst, data = sparse_matrix, label = output_vector)
  
  # Cleaning for better display
  importanceClean <- importanceRaw[,`:=`(Cover=NULL, Frequency=NULL)]
  
  head(importanceClean)
  
  ##             Feature        Split       Gain RealCover RealCover %
  ## 1: TreatmentPlacebo -1.00136e-05 0.28575061         7   0.2500000
  ## 2:              Age         61.5 0.16374034        12   0.4285714
  ## 3:              Age           39 0.08705750         8   0.2857143
  ## 4:              Age         57.5 0.06947553        11   0.3928571
  ## 5:          SexMale -1.00136e-05(==1) 0.04874405         4   0.1428571
  ## 6:              Age         53.5 0.04620627        10   0.3571429
  ```

  * RealCover 满足Feature<Split & Y ==1 的样本个数.

  * RealCover % = RealCover / Y==1的总个数.

  * Gain-区分能力， RealCover-对于Y==1的影响程度.

    