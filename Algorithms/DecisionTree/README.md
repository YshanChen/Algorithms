## ID3 & C4.5

#### 1. 一些细节记录

1. 特征分裂时，不按剩余特征类别分裂(unique)，按全部类别分裂(cat.catogaries)。否则，将导致新数据集出现划分不到某一类的情况。
2. 如果某特征出现训练集中未包含的属性，则程序会报错，待完善；
3. 返回样本最多的类别时，返回的是当前节点下最多的类别，不是根节点的最多类别。
4. 分裂停止条件：
   1. Y只存在一种取值：则取该值。
   2. 信息增益/信息增益率小于设定的阈值：取当前结点下样本最多的类别。(min_impurity_split)
   3. 内部节点待分裂样本小于最小阈值：取样本最多的类别。（min_samples_split）
   4. 叶子结点最小样本数：若分裂后，两个子节点的样本数小于设定的阈值，则不分裂。取当前节点下样本最多的类别。（min_samples_leaf）
   5. 可分裂特征小于划分时考虑的最大特征数：取当前节点下样本最多的类别。（max_features）
   6. 树的最大深度：取当前结点下样本最多的类别。（max_depth）

   附：skilearn中对于 min_samples_split 和 min_samples_leaf 的描述：
   通过使用 min_samples_split 和 min_samples_leaf 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。所以尝试 min_samples_leaf=5 作为初始值。如果样本的变化量很大，可以使用浮点数作为这两个参数中的百分比。两者之间的主要区别在于 min_samples_leaf 保证叶结点中最少的采样数，而 min_samples_split 可以创建任意小的叶子，尽管在文献中 min_samples_split 更常见。

5. 如果IV=0，则将信息增益率赋值为0；
6. 预测集中，'Parch'存在类别'9'，但训练集不存在该类别，故模型未考虑新增类别问题。解决方案具体见《机器学习》西瓜书。

#### 2. 几个重点

1. 信息增益率对可取值少数目较少的特征有所偏好，C4.5并不是直接选择信息增益率最大的候选划分特征，而是使用了一个启发式：先从候选划分特征中找出信息增益高于平均水平的属性，再从中选取信息增益率最高的。
2. C4.5算法对连续值采用二分法。
3. 与离散特征不同，若当前结点划分特征为连续特征，该特征还可作为其后代结点的划分属性。
4. 缺失值的处理，需解决两个问题：
   1. 如何在叶特征值缺失的情况下进行特征选择？
   2. 给定选择的特征，若样本在该特征上的值缺失，如何对样本进行划分？

#### 3. Todo List

1. 连续值的处理：详见CART算法实现。
2. 缺失值的处理：详见CART算法实现。
3. 剪枝。



## CART

#### 0. 概述

实现了回归和分类两类目标的算法。
西瓜数据集(分类)、泰坦尼克号数据集(分类)、波士顿房价数据集(回归)测试通过，效果能达到正常水平。

#### 1. 几个重点

1. CART假设决策树是二叉树。
2. 对回归树用平方误差(square loss)最小化准则，对分类树用基尼指数(Gini index)最小化准则，进行特征选择，生成二叉树。
3. 分类问题的节点分裂特征的处理：
   * ID3：根据信息增益最大原则，找到最优特征即可。只能处理categorical，对每个分类进行多叉分裂。
   * C4.5：根据信息增益率最大原则。处理categorical同ID3。处理numeric需找到最优特征和最优切分点（采用二分法）。遍历所有候选划分点，选择信息增益(率)最大的作为该特征的信息增益(率)，该划分点作为最优划分点。
   * CART：根据基尼指数最小原则。处理categorical和numerical均采用二分法，均需要找到最优特征和最优切分点。
4. 回归问题的节点分裂的处理：
   1. 针对特征x:
        1. 遍历：所有划分点；
        2. 计算：经划分点后的子节点的输出 $c1 \ and \ c2, \ \ c = avg(y)$;
        3. 计算：该划分点下的平方损失: $\min \limits_{j,s} [\min \limits_{c_1} \sum_{x_i \in R_1(j,s)} (y_i-c_1)^2 + \min \limits_{c_2} \sum_{x_i \in R_2(j,s)} (y_i-c_2)^2]$；
        4. 筛选：最小平方损失对应的划分点作为该特征最优划分点，该平方损失作为该特征的损失；
   2. 遍历：所有特征，计算划分点，平方损失；
   3. 筛选：最小平方损失对应的特征作为分裂特征，划分点为其最优划分点，平方损失为此次分裂的损失；
4. 对于特征能否重复利用的问题。
   * 应该考虑的是在当前分支下，该特征能否可再次划分（或是是否还有可利用的价值），而不能再次划分的意思就是在当前分支内，该特征只有唯一的值（或者按照C4.5中的定义：该特征的分裂信息为0）。
   * ID3和C4.5在离散特征上都是多叉树，就是按照该特征的**全部特征值**进行分裂，那么显然分裂后的任意一个分支内特征只有唯一的值，所以该特征消耗掉了。
   * C4.5是将连续特征进行排序，按照相邻离散值的中点进行分裂。如果该连续特征只有两个离散值，那么一次分裂就会消耗掉这个特征；如果有多个离散值，那么可以经受多次分裂。
   * CART算法是二叉树，也就是**无论$X_i$是连续特征还是离散特征都是二叉分裂**的。那么如果$X_i$有多于2个离散值，即便Xi是离散特征，也可以经受多次分裂，也就是你说的能被重复利用。
5. ID3&C4.5的剪枝，与CART不同。


#### 2. Todo List

1. 缺失值的处理。
2. 剪枝。
3. 样本并行化预测。

