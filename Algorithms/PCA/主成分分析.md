# 主成分分析(PCA)

#### 1. 算法介绍

主成分分析（Principal Components Analysis），简称PCA，是一种数据降维技术，用于数据预处理。理解PCA算法，对实现白化算法有很大的帮助，很多算法都先用白化算法作预处理步骤。

假如我们正在训练的16x16灰度值图像，记为一个256维向量 ![\textstyle x \in \Re^{256}](http://deeplearning.stanford.edu/wiki/images/math/3/e/c/3ec732c534e730334fbe728ae49c8fce.png) ，其中特征值 ![\textstyle x_j](http://deeplearning.stanford.edu/wiki/images/math/b/d/f/bdf5b20642553027712d5b5240b31cf3.png) 对应每个像素的亮度值。由于相邻像素间的相关性，PCA算法可以将输入向量转换为一个维数低很多的近似向量，而且误差非常小。

#### 2. 算法步骤

1. **零均值化**

   零均值化就是求每一列特征的平均值，然后就该特征上的所有数都减去这个均值。也就是说，这里零均值化是对每一个特征而言的零均值化，使得每个特征的均值变成0。		

2. **求协方差矩阵**

3. **求特征值、特征向量**

4. **选择主成分个数，即保留方差值较大的前N个特征**

#### 3. 实例和数学背景

**实例**

-----------

在我们的实例中，使用的输入数据集表示为 ![\textstyle \{x^{(1)}, x^{(2)}, \ldots, x^{(m)}\}](http://deeplearning.stanford.edu/wiki/images/math/b/b/f/bbfa674fd83f37c2c66867d7e0cc264a.png) ，维度 ![\textstyle n=2](http://deeplearning.stanford.edu/wiki/images/math/b/1/9/b1993eef97e184af6b11db01e694445f.png) 即 ![\textstyle x^{(i)} \in \Re^2](http://deeplearning.stanford.edu/wiki/images/math/1/b/a/1babb19c8b06f9a7bd624fa60f29d5fb.png) 。假设我们想把数据从2维降到1维。（在实际应用中，我们也许需要把数据从256维降到50维；在这里使用低维数据，主要是为了更好地可视化算法的行为）。下图是我们的数据集：

[![PCA-rawdata.png](http://deeplearning.stanford.edu/wiki/images/thumb/b/ba/PCA-rawdata.png/600px-PCA-rawdata.png)](http://deeplearning.stanford.edu/wiki/index.php/File:PCA-rawdata.png)

这些数据已经进行了预处理，使得每个特征 ![\textstyle x_1](http://deeplearning.stanford.edu/wiki/images/math/f/a/7/fa7eebd32aa8c9cdae2b2aacbc324331.png) 和 ![\textstyle x_2](http://deeplearning.stanford.edu/wiki/images/math/7/6/8/76879b7da23d4991dfcb03323403c152.png) 具有相同的均值（零）和方差。

为方便展示，根据 ![\textstyle x_1](http://deeplearning.stanford.edu/wiki/images/math/f/a/7/fa7eebd32aa8c9cdae2b2aacbc324331.png) 值的大小，我们将每个点分别涂上了三种颜色之一，但该颜色并不用于算法而仅用于图解。

PCA算法将寻找一个低维空间来投影我们的数据。从下图中可以看出， ![\textstyle u_1](http://deeplearning.stanford.edu/wiki/images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 是数据变化的主方向，而 ![\textstyle u_2](http://deeplearning.stanford.edu/wiki/images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 是次方向。

[![PCA-u1.png](http://deeplearning.stanford.edu/wiki/images/thumb/b/b4/PCA-u1.png/600px-PCA-u1.png)](http://deeplearning.stanford.edu/wiki/index.php/File:PCA-u1.png)

也就是说，数据在 ![\textstyle u_1](http://deeplearning.stanford.edu/wiki/images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 方向上的变化要比在 ![\textstyle u_2](http://deeplearning.stanford.edu/wiki/images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 方向上大。为更形式化地找出方向 ![\textstyle u_1](http://deeplearning.stanford.edu/wiki/images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 和 ![\textstyle u_2](http://deeplearning.stanford.edu/wiki/images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) ，我们首先计算出矩阵 ![\textstyle \Sigma](http://deeplearning.stanford.edu/wiki/images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png) ，如下所示：


假设 ![\textstyle x](http://deeplearning.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 的均值为零，那么 ![\textstyle \Sigma](http://deeplearning.stanford.edu/wiki/images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png) 就是x的协方差矩阵。（符号 ![\textstyle \Sigma](http://deeplearning.stanford.edu/wiki/images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png) ，读"Sigma"，是协方差矩阵的标准符号。虽然看起来与求和符号 ![\sum_{i=1}^n i](http://deeplearning.stanford.edu/wiki/images/math/7/3/b/73b577d2b026ab8f8fb733953266427e.png) 比较像，但它们其实是两个不同的概念。）

可以证明，数据变化的主方向 ![\textstyle u_1](http://deeplearning.stanford.edu/wiki/images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 就是协方差矩阵 ![\textstyle \Sigma](http://deeplearning.stanford.edu/wiki/images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png) 的主特征向量，而 ![\textstyle u_2](http://deeplearning.stanford.edu/wiki/images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 是次特征向量。

你可以通过标准的数值线性代数运算软件求得特征向量（见实现说明）.我们先计算出协方差矩阵![\textstyle \Sigma](http://deeplearning.stanford.edu/wiki/images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png)的特征向量，按列排放，而组成矩阵![\textstyle U](http://deeplearning.stanford.edu/wiki/images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png)：
$$
U = \left[
\begin{matrix}
 \vdots      & \vdots      &  & \vdots      \\
 u_1      & u_2      & \cdots & u_n      \\
 \vdots & \vdots &  & \vdots \\
\end{matrix}
\right]
$$


此处， ![\textstyle u_1](http://deeplearning.stanford.edu/wiki/images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 是主特征向量（对应最大的特征值）， ![\textstyle u_2](http://deeplearning.stanford.edu/wiki/images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 是次特征向量。以此类推，另记 ![\textstyle \lambda_1, \lambda_2, \ldots, \lambda_n](http://deeplearning.stanford.edu/wiki/images/math/d/2/b/d2b02582947d98e3be81be3d1e684f28.png) 为相应的特征值。

在本例中，向量 ![\textstyle u_1](http://deeplearning.stanford.edu/wiki/images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 和 ![\textstyle u_2](http://deeplearning.stanford.edu/wiki/images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 构成了一个新基，可以用来表示数据。令 ![\textstyle x \in \Re^2](http://deeplearning.stanford.edu/wiki/images/math/b/2/6/b260df225bb49f3ff776b17a50cd20d3.png) 为训练样本，那么 ![\textstyle u_1^Tx](http://deeplearning.stanford.edu/wiki/images/math/7/c/0/7c0e7fb10fb6e75bad211b2f2070c24c.png) 就是样本点 ![\textstyle x](http://deeplearning.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 在维度 ![\textstyle u_1](http://deeplearning.stanford.edu/wiki/images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 上的投影的长度（幅值）。同样的， ![\textstyle u_2^Tx](http://deeplearning.stanford.edu/wiki/images/math/3/8/9/389b689de5736f95b05c3be9c373b95a.png) 是 ![\textstyle x](http://deeplearning.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 投影到 ![\textstyle u_2](http://deeplearning.stanford.edu/wiki/images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 维度上的幅值。

**旋转数据**

---------

至此，我们可以把 ![\textstyle x](http://deeplearning.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 用 ![\textstyle (u_1, u_2)](http://deeplearning.stanford.edu/wiki/images/math/0/3/2/0329a7ca7eca352beded9f24406d34fe.png) 基表达为：
$$
x_{rot} = {U^T}x =\left[
\begin{matrix}
 u_1^Tx \\
 u_2^Tx \\
\end{matrix}
\right]
$$


（下标“rot”来源于单词“rotation”，意指这是原数据经过旋转（也可以说成映射）后得到的结果）

对数据集中的每个样本 ![\textstyle i](http://deeplearning.stanford.edu/wiki/images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png) 分别进行旋转： ![\textstyle x_{\rm rot}^{(i)} = U^Tx^{(i)}](http://deeplearning.stanford.edu/wiki/images/math/c/d/0/cd047246fd68f6d52b2fd068e063c0ef.png) for every ![\textstyle i](http://deeplearning.stanford.edu/wiki/images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png) ，然后把变换后的数据 ![\textstyle x_{\rm rot}](http://deeplearning.stanford.edu/wiki/images/math/1/7/0/170047e804738636731477291969d554.png) 显示在坐标图上，可得：

[![PCA-rotated.png](http://deeplearning.stanford.edu/wiki/images/thumb/1/12/PCA-rotated.png/600px-PCA-rotated.png)](http://deeplearning.stanford.edu/wiki/index.php/File:PCA-rotated.png)

这就是把训练数据集旋转到 ![\textstyle u_1](http://deeplearning.stanford.edu/wiki/images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png)，![\textstyle u_2](http://deeplearning.stanford.edu/wiki/images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 基后的结果。一般而言，运算 ![\textstyle U^Tx](http://deeplearning.stanford.edu/wiki/images/math/e/0/a/e0aec5d033ea89dc9bd9c83bc2b4edec.png) 表示旋转到基 ![\textstyle u_1](http://deeplearning.stanford.edu/wiki/images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png),![\textstyle u_2](http://deeplearning.stanford.edu/wiki/images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png), ...,![\textstyle u_n](http://deeplearning.stanford.edu/wiki/images/math/0/b/e/0be80bb4e50881840b92fb8331ef2bbd.png) 之上的训练数据。矩阵 ![\textstyle U](http://deeplearning.stanford.edu/wiki/images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png) 有正交性，即满足![\textstyle U^TU = UU^T = I](http://deeplearning.stanford.edu/wiki/images/math/a/8/2/a825fd85c23ffa9b851fb64c9c816ad6.png) ，所以若想将旋转后的向量 ![\textstyle x_{\rm rot}](http://deeplearning.stanford.edu/wiki/images/math/1/7/0/170047e804738636731477291969d554.png) 还原为原始数据 ![\textstyle x](http://deeplearning.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) ，将其左乘矩阵![\textstyle U](http://deeplearning.stanford.edu/wiki/images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png)即可： ![\textstyle x=U x_{\rm rot}](http://deeplearning.stanford.edu/wiki/images/math/f/a/a/faada910e82b90d1c221943616cc85ab.png) , 验算一下： ![\textstyle U x_{\rm rot} =  UU^T x = x](http://deeplearning.stanford.edu/wiki/images/math/a/5/f/a5fa6224542f5b2871447986260574d2.png).

**数据降维**

---------

数据的主方向就是旋转数据的第一维 ![\textstyle x_{{\rm rot},1}](http://deeplearning.stanford.edu/wiki/images/math/0/0/6/0066d1e2efa2f0019a3dfd3469862934.png) 。因此，若想把这数据降到一维，可令：


更一般的，假如想把数据 ![\textstyle x \in \Re^n](http://deeplearning.stanford.edu/wiki/images/math/9/e/b/9ebd39996afb169318c1dd5fb1503b17.png) 降到 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 维表示 ![\textstyle \tilde{x} \in \Re^k](http://deeplearning.stanford.edu/wiki/images/math/2/1/3/21337248295f42f7fe18d9a9b3da57b1.png) （令 ![\textstyle k < n](http://deeplearning.stanford.edu/wiki/images/math/8/7/b/87b6508de7e0487479389cff2b5fa91a.png) ）,只需选取 ![\textstyle x_{\rm rot}](http://deeplearning.stanford.edu/wiki/images/math/1/7/0/170047e804738636731477291969d554.png) 的前 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个成分，分别对应前 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个数据变化的主方向。

PCA的另外一种解释是：![\textstyle x_{\rm rot}](http://deeplearning.stanford.edu/wiki/images/math/1/7/0/170047e804738636731477291969d554.png) 是一个 ![\textstyle n](http://deeplearning.stanford.edu/wiki/images/math/0/c/5/0c59de0fa75c1baa1c024aabfa43b2e3.png) 维向量，其中前几个成分可能比较大（例如，上例中大部分样本第一个成分 ![\textstyle x_{{\rm rot},1}^{(i)} = u_1^Tx^{(i)}](http://deeplearning.stanford.edu/wiki/images/math/8/0/e/80ebba0459d97a31a03e9de6b0957c31.png) 的取值相对较大），而后面成分可能会比较小（例如，上例中大部分样本的 ![\textstyle x_{{\rm rot},2}^{(i)} = u_2^Tx^{(i)}](http://deeplearning.stanford.edu/wiki/images/math/4/6/8/468a726aaaea7f4aabbeb8a2e1966aae.png) 较小）。

PCA算法做的其实就是丢弃 ![\textstyle x_{\rm rot}](http://deeplearning.stanford.edu/wiki/images/math/1/7/0/170047e804738636731477291969d554.png) 中后面（取值较小）的成分，就是将这些成分的值近似为零。具体的说，设 ![\textstyle \tilde{x}](http://deeplearning.stanford.edu/wiki/images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 是 ![\textstyle x_{{\rm rot}}](http://deeplearning.stanford.edu/wiki/images/math/7/7/4/774d8fa9b41f58dfc57cebb419e0de60.png) 的近似表示，那么将 ![\textstyle x_{{\rm rot}}](http://deeplearning.stanford.edu/wiki/images/math/7/7/4/774d8fa9b41f58dfc57cebb419e0de60.png) 除了前 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个成分外，其余全赋值为零，就得到：


在本例中，可得 ![\textstyle \tilde{x}](http://deeplearning.stanford.edu/wiki/images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 的点图如下（取 ![\textstyle n=2, k=1](http://deeplearning.stanford.edu/wiki/images/math/9/4/b/94b3c8bb8f57addfc319217446a14d56.png) ）：

[![PCA-xtilde.png](http://deeplearning.stanford.edu/wiki/images/thumb/2/27/PCA-xtilde.png/600px-PCA-xtilde.png)](http://deeplearning.stanford.edu/wiki/index.php/File:PCA-xtilde.png)

然而，由于上面 ![\textstyle \tilde{x}](http://deeplearning.stanford.edu/wiki/images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 的后![\textstyle n-k](http://deeplearning.stanford.edu/wiki/images/math/7/4/2/742be0073915ce28ed208c2d5c83fc56.png)项均为零，没必要把这些零项保留下来。所以，我们仅用前 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个（非零）成分来定义 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 维向量 ![\textstyle \tilde{x}](http://deeplearning.stanford.edu/wiki/images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 。

这也解释了我们为什么会以 ![\textstyle u_1, u_2, \ldots, u_n](http://deeplearning.stanford.edu/wiki/images/math/d/5/2/d52832ed87962d3ece3043ddae3150a7.png) 为基来表示数据：要决定保留哪些成分变得很简单，只需取前 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个成分即可。这时也可以说，我们“保留了前 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个PCA（主）成分”。

 **还原近似数据**

------------------

现在，我们得到了原始数据 ![\textstyle x \in \Re^n](http://deeplearning.stanford.edu/wiki/images/math/9/e/b/9ebd39996afb169318c1dd5fb1503b17.png) 的低维“压缩”表征量 ![\textstyle \tilde{x} \in \Re^k](http://deeplearning.stanford.edu/wiki/images/math/2/1/3/21337248295f42f7fe18d9a9b3da57b1.png) ， 反过来，如果给定 ![\textstyle \tilde{x}](http://deeplearning.stanford.edu/wiki/images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) ，我们应如何还原原始数据 ![\textstyle x](http://deeplearning.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 呢？根据上面可知，要转换回来，只需 ![\textstyle x = U x_{\rm rot}](http://deeplearning.stanford.edu/wiki/images/math/f/a/a/faada910e82b90d1c221943616cc85ab.png) 即可。进一步，我们把 ![\textstyle \tilde{x}](http://deeplearning.stanford.edu/wiki/images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 看作将 ![\textstyle x_{\rm rot}](http://deeplearning.stanford.edu/wiki/images/math/1/7/0/170047e804738636731477291969d554.png) 的最后 ![\textstyle n-k](http://deeplearning.stanford.edu/wiki/images/math/7/4/2/742be0073915ce28ed208c2d5c83fc56.png) 个元素被置0所得的近似表示，因此如果给定 ![\textstyle \tilde{x} \in \Re^k](http://deeplearning.stanford.edu/wiki/images/math/2/1/3/21337248295f42f7fe18d9a9b3da57b1.png) ，可以通过在其末尾添加 ![\textstyle n-k](http://deeplearning.stanford.edu/wiki/images/math/7/4/2/742be0073915ce28ed208c2d5c83fc56.png) 个0来得到对 ![\textstyle x_{\rm rot} \in \Re^n](http://deeplearning.stanford.edu/wiki/images/math/f/c/5/fc52a57fe97de0666dc2857bde2df153.png) 的近似，最后，左乘 ![\textstyle U](http://deeplearning.stanford.edu/wiki/images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png) 便可近似还原出原数据 ![\textstyle x](http://deeplearning.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 。具体来说，计算如下：
$$
\hat{x} = U\left[
\begin{matrix}
 x_{rot,1} \\
\vdots\\
x_{rot,k} \\
0 \\
\vdots \\
0\\
\end{matrix}
\right]= \sum^{k}_{i=1}{u_ix_{rot,i}}
$$
上面的等式基于[先前](http://deeplearning.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90#.E5.AE.9E.E4.BE.8B.E5.92.8C.E6.95.B0.E5.AD.A6.E8.83.8C.E6.99.AF)对 ![\textstyle U](http://deeplearning.stanford.edu/wiki/images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png) 的定义。在实现时，我们实际上并不先给 ![\textstyle \tilde{x}](http://deeplearning.stanford.edu/wiki/images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 填0然后再左乘 ![\textstyle U](http://deeplearning.stanford.edu/wiki/images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png) ，因为这意味着大量的乘0运算。我们可用 ![\textstyle \tilde{x} \in \Re^k](http://deeplearning.stanford.edu/wiki/images/math/2/1/3/21337248295f42f7fe18d9a9b3da57b1.png) 来与 ![\textstyle U](http://deeplearning.stanford.edu/wiki/images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png) 的前 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 列相乘，即上式中最右项，来达到同样的目的。将该算法应用于本例中的数据集，可得如下关于重构数据 ![\textstyle \hat{x}](http://deeplearning.stanford.edu/wiki/images/math/2/9/0/29035749c12270bcc8de7e36bc459ece.png) 的点图：

[![PCA-xhat.png](http://deeplearning.stanford.edu/wiki/images/thumb/5/52/PCA-xhat.png/600px-PCA-xhat.png)](http://deeplearning.stanford.edu/wiki/index.php/File:PCA-xhat.png)

由图可见，我们得到的是**对原始数据集的一维近似重构。**

在训练自动编码器或其它无监督特征学习算法时，算法运行时间将依赖于输入数据的维数。若用 ![\textstyle \tilde{x} \in \Re^k](http://deeplearning.stanford.edu/wiki/images/math/2/1/3/21337248295f42f7fe18d9a9b3da57b1.png) 取代 ![\textstyle x](http://deeplearning.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 作为输入数据，那么算法就可使用低维数据进行训练，运行速度将显著加快。对于很多数据集来说，低维表征量 ![\textstyle \tilde{x}](http://deeplearning.stanford.edu/wiki/images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 是原数据集的极佳近似，因此在这些场合使用PCA是很合适的，它引入的近似误差的很小，却可显著地提高你算法的运行速度。

 

#### 4. 选择主成分K个数

我们该如何选择 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) ，即保留多少个PCA主成分？对于高维数据来说，做这个决定就没那么简单：如果 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 过大，数据压缩率不高，在极限情况 ![\textstyle k=n](http://deeplearning.stanford.edu/wiki/images/math/e/3/6/e36b85de9c58866d875f20cbf6fc5f5b.png) 时，等于是在使用原始数据（只是旋转投射到了不同的基）；相反地，如果 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 过小，那数据的近似误差太太。

决定 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 值时，我们通常会考虑不同 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 值可保留的方差百分比。具体来说，如果 ![\textstyle k=n](http://deeplearning.stanford.edu/wiki/images/math/e/3/6/e36b85de9c58866d875f20cbf6fc5f5b.png) ，那么我们得到的是对数据的完美近似，也就是保留了100%的方差，即原始数据的所有变化都被保留下来；相反，如果 ![\textstyle k=0](http://deeplearning.stanford.edu/wiki/images/math/2/a/2/2a27a4874f5739de5d2947d12ac81d4b.png) ，那等于是使用零向量来逼近输入数据，也就是只有0%的方差被保留下来。

一般而言，设 ![\textstyle \lambda_1, \lambda_2, \ldots, \lambda_n](http://deeplearning.stanford.edu/wiki/images/math/d/2/b/d2b02582947d98e3be81be3d1e684f28.png) 表示协方差矩阵 ![\textstyle \Sigma](http://deeplearning.stanford.edu/wiki/images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png) 的特征值（按由大到小顺序排列），使得 ![\textstyle \lambda_j](http://deeplearning.stanford.edu/wiki/images/math/c/8/5/c851ef66a35ee95db0b63a592963ca77.png) 为对应于特征向量 ![\textstyle u_j](http://deeplearning.stanford.edu/wiki/images/math/d/1/7/d175faaca44b996970abf70b700a94f1.png) 的特征值。那么如果我们保留前 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个成分，则保留的方差百分比可计算为：

$\cfrac{\sum^{k}_{j=1}{\lambda_j}}{\sum^{n}_{j=1}{\lambda_j}}$

很容易证明，![\textstyle \lambda_j = \sum_{i=1}^m x_{{\rm rot},j}^2](http://deeplearning.stanford.edu/wiki/images/math/9/7/e/97ecfffd8596d26deed9542b64cd6712.png) 。因此，如果 ![\textstyle \lambda_j \approx 0](http://deeplearning.stanford.edu/wiki/images/math/6/7/1/6716d88c3c1a368824d188c8b9b6b589.png) ，则说明 ![\textstyle x_{{\rm rot},j}](http://deeplearning.stanford.edu/wiki/images/math/e/8/4/e84f84acac7b07e18a42a8e91b4433bc.png) 也就基本上接近于0，所以用0来近似它并不会产生多大损失。这也解释了为什么要保留前面的主成分（对应的 ![\textstyle \lambda_j](http://deeplearning.stanford.edu/wiki/images/math/c/8/5/c851ef66a35ee95db0b63a592963ca77.png) 值较大）而不是末尾的那些。 这些前面的主成分 ![\textstyle x_{{\rm rot},j}](http://deeplearning.stanford.edu/wiki/images/math/e/8/4/e84f84acac7b07e18a42a8e91b4433bc.png) 变化性更大，取值也更大，如果将其设为0势必引入较大的近似误差。

以处理图像数据为例，一个惯常的经验法则是选择 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 以保留99%的方差，对其它应用，如不介意引入稍大的误差，有时也保留90-98%的方差范围。若向他人介绍PCA算法详情，告诉他们你选择的 ![\textstyle k](http://deeplearning.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 保留了95%的方差，比告诉他们你保留了前120个（或任意某个数字）主成分更好理解。



#### 5. Sklearn.decomposition PCA

**1、函数原型及参数说明**

`sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False)`

参数说明：

n_components:

> 意义：PCA算法中所要保留的主成分个数n，也即保留下来的特征个数n
>
> 类型：int 或者 string，缺省时默认为None，所有成分被保留。赋值为int，比如n_components=1，将把原始数据降到一个维度。赋值为string，比如n_components='mle'，将自动选取特征个数n，使得满足所要求的方差百分比。

copy:

> 类型：bool，True或者False，缺省时默认为True。
>
> 意义：表示是否在运行算法时，将原始训练数据复制一份。若为True，则运行PCA算法后，原始训练数据的值不会有任何改变，因为是在原始数据的副本上进行运算；若为False，则运行PCA算法后，原始训练数据的值会改，因为是在原始数据上进行降维计算。

whiten:

> 类型：bool，缺省时默认为False
>
> 意义：白化，使得每个特征具有相同的方差。关于“白化”，可参考：[Ufldl教程](http://deeplearning.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96)

**2、PCA对象的属性**

components_ ：返回具有最大方差的成分。

explained_variance_ratio_：返回 所保留的n个成分各自的方差百分比。

n_components_：返回所保留的成分个数n。

**3、PCA对象的方法**

- fit(X,y=None)

fit()可以说是scikit-learn中通用的方法，每个需要训练的算法都会有fit()方法，它其实就是算法中的“训练”这一步骤。因为PCA是无监督学习算法，此处y自然等于None。

fit(X)，表示用数据X来训练PCA模型。

函数返回值：调用fit方法的对象本身。比如pca.fit(X)，表示用X对pca这个对象进行训练。

- fit_transform(X)

用X来训练PCA模型，同时返回降维后的数据。

newX=pca.fit_transform(X)，newX就是降维后的数据。

- inverse_transform()

将降维后的数据转换成原始数据，X=pca.inverse_transform(newX)

- transform(X)

将数据X转换成降维后的数据。当模型训练好后，对于新输入的数据，都可以用transform方法来降维。

此外，还有get_covariance()、get_precision()、get_params(deep=True)、score(X, y=None)等方法，以后用到再补充吧。

 **4、example**

以一组二维的数据data为例，data如下，一共12个样本（x,y），其实就是分布在直线y=x上的点，并且聚集在x=1、2、3、4上，各3个。

```python
>>> data
array([[ 1.  ,  1.  ],
       [ 0.9 ,  0.95],
       [ 1.01,  1.03],
       [ 2.  ,  2.  ],
       [ 2.03,  2.06],
       [ 1.98,  1.89],
       [ 3.  ,  3.  ],
       [ 3.03,  3.05],
       [ 2.89,  3.1 ],
       [ 4.  ,  4.  ],
       [ 4.06,  4.02],
       [ 3.97,  4.01]])
```

data这组数据，有两个特征，因为两个特征是近似相等的，所以用一个特征就能表示了，即可以降到一维。下面就来看看怎么用sklearn中的PCA算法包。

（1）n_components设置为1，copy默认为True，可以看到原始数据data并未改变，newData是一维的，并且明显地将原始数据分成了四类。

>>> from sklearn.decomposition import PCA 
>>> pca=PCA(n_components=1)
>>> newData=pca.fit_transform(data)
>>> newData
>>> array([[-2.12015916],
>>>    [-2.22617682],
>>>    [-2.09185561],
>>>    [-0.70594692],
>>>    [-0.64227841],
>>>    [-0.79795758],
>>>    [ 0.70826533],
>>>    [ 0.76485312],
>>>    [ 0.70139695],
>>>    [ 2.12247757],
>>>    [ 2.17900746],
>>>    [ 2.10837406]])
>>> data
>>> array([[ 1.  ,  1.  ],
>>>    [ 0.9 ,  0.95],
>>>    [ 1.01,  1.03],
>>>    [ 2.  ,  2.  ],
>>>    [ 2.03,  2.06],
>>>    [ 1.98,  1.89],
>>>    [ 3.  ,  3.  ],
>>>    [ 3.03,  3.05],
>>>    [ 2.89,  3.1 ],
>>>    [ 4.  ,  4.  ],
>>>    [ 4.06,  4.02],
>>>    [ 3.97,  4.01]])

（2）将copy设置为False，原始数据data将发生改变。

>>> pca=PCA(n_components=1,copy=False)
>>> newData=pca.fit_transform(data)
>>> data
>>> array([[-1.48916667, -1.50916667],
>>>    [-1.58916667, -1.55916667],
>>>    [-1.47916667, -1.47916667],
>>>    [-0.48916667, -0.50916667],
>>>    [-0.45916667, -0.44916667],
>>>    [-0.50916667, -0.61916667],
>>>    [ 0.51083333,  0.49083333],
>>>    [ 0.54083333,  0.54083333],
>>>    [ 0.40083333,  0.59083333],
>>>    [ 1.51083333,  1.49083333],
>>>    [ 1.57083333,  1.51083333],
>>>    [ 1.48083333,  1.50083333]])

（3）n_components设置为'mle'，看看效果，自动降到了1维。

>>> pca=PCA(n_components='mle')
>>> newData=pca.fit_transform(data)
>>> newData
>>> array([[-2.12015916],
>>>    [-2.22617682],
>>>    [-2.09185561],
>>>    [-0.70594692],
>>>    [-0.64227841],
>>>    [-0.79795758],
>>>    [ 0.70826533],
>>>    [ 0.76485312],
>>>    [ 0.70139695],
>>>    [ 2.12247757],
>>>    [ 2.17900746],
>>>    [ 2.10837406]])

（4）对象的属性值

>>> pca.n_components
>>> 1
>>> pca.explained_variance_ratio_
>>> array([ 0.99910873])
>>> pca.explained_variance_
>>> array([ 2.55427003])
>>> pca.get_params
>>> <bound method PCA.get_params of PCA(copy=True, n_components=1, whiten=False)>

我们所训练的pca对象的n_components值为1，即保留1个特征，该特征的方差为2.55427003，占所有特征的方差百分比为0.99910873，意味着几乎保留了所有的信息。get_params返回各个参数的值。

（5）对象的方法

> > > newA=pca.transform(A)

对新的数据A，用已训练好的pca模型进行降维。

>>> pca.set_params(copy=False)
>>> PCA(copy=False, n_components=1, whiten=False)



参考：

1. http://deeplearning.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90
2. https://blog.csdn.net/u012162613/article/details/42177327
3. https://blog.csdn.net/u012162613/article/details/42192293





