## ID3 & C4.5

#### 1. 一些细节记录

1. 特征分裂时，不按剩余特征类别分裂(unique)，按全部类别分裂(cat.catogaries)。否则，将导致新数据集出现划分不到某一类的情况。
2. 如果某特征出现训练集中未包含的属性，则程序会报错；
3. 返回样本最多的类别时，返回的是当前结点下最多的类别，不是根结点的最多类别。
4. 分裂停止条件：
   1. Y只存在一种取值：则取该值。
   2. 信息增益/信息增益率小于设定的阈值：取当前结点下样本最多的类别。(min_impurity_split)
   3. 内部结点待分裂样本小于最小阈值：取分裂前的样本最多的类别。（min_samples_split）
   4. 可分裂特征小于划分时考虑的最大特征数：取当前结点下样本最多的类别。（max_features）（待修改）
   5. 树的最大深度：取当前结点下样本最多的类别。（max_depth）
   6. 叶子结点最小样本数：取当前结点下样本最多的类别。（min_samples_leaf）
5. 如果IV=0，则将信息增益率赋值为0；
6. 预测集中，'Parch'存在类别'9'，但训练集不存在该类别，故模型未考虑新增类别问题。解决方案具体见《机器学习》西瓜书。



#### 2. 几个重点

1. 信息增益率对可取值少数目较少的特征有所偏好，C4.5并不是直接选择信息增益率最大的候选划分特征，而是使用了一个启发式：先从候选划分特征中找出信息增益高于平均水平的属性，再从中选取信息增益率最高的。
2. C4.5算法对连续值采用二分法。
3. 与离散特征不同，若当前结点划分特征为连续特征，该特征还可作为其后代结点的划分属性。
4. 缺失值的处理，需解决两个问题：
   1. 如何在叶特征值缺失的情况下进行特征选择？
   2. 给定选择的特征，若样本在该特征上的值缺失，如何对样本进行划分？



#### 3. Todo List

1. 连续值的处理：详见CART算法实现。
2. 缺失值的处理：详见CART算法实现。
3. 剪枝。



## CART

#### 1. 几个重点

1. CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”。
2. 对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树。
3. * 对于ID3，只需要找到最优特征即可。
   * 对于C4.5，处理categorical只需找到最优特征即可。处理numeric需找到最优特征和最优切分点（二分法）。遍历所有候选划分点，选择信息增益(率)最大的作为该特征的信息增益(率)，该划分点作为最优划分点。
   * 对于CART，均需要找到最优特征和最优切分点。
4. 对于特征能否重复利用的问题。
   * 应该考虑的是在当前分支下，该特征能否可再次划分（或是是否还有可利用的价值），而不能再次划分的意思就是在当前分支内，该特征只有唯一的值（或者按照C4.5中的定义：该特征的分裂信息为0）。
   * ID3和C4.5在离散特征上都是多叉树，就是按照该特征的**全部特征值**进行分裂，那么显然分裂后的任意一个分支内特征只有唯一的值，所以该特征消耗掉了。
   * C4.5是将连续特征进行排序，按照相邻离散值的中点进行分裂。如果该连续特征只有两个离散值，那么一次分裂就会消耗掉这个特征；如果有多个离散值，那么可以经受多次分裂。
   * CART算法是二叉树，也就是**无论$X_i$是连续特征还是离散特征都是二叉分裂**的。那么如果$X_i$有多于2个离散值，即便Xi是离散特征，也可以经受多次分裂，也就是你说的能被重复利用。
5. ID3&C4.5的剪枝，与CART不同。



#### 2. Todo List

1. 缺失值的处理。
2. 剪枝。
3. 样本并行化预测。

